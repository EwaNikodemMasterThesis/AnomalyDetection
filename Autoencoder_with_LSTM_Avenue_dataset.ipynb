{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":446,"status":"ok","timestamp":1639557267173,"user":{"displayName":"Sebastian Mazur","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13209436418749151775"},"user_tz":-60},"id":"XcYJWl4SWkkr","outputId":"04691950-f12a-445e-9267-3e9c2d5caaf4","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: docelowa ścieżka „UCSD” już istnieje i nie jest pustym katalogiem.\r\n"]}],"source":["!git clone https://github.com/EwaNikodemMasterThesis/AnomalyDetection.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RWSJpsyKqHjH","scrolled":false},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Scm0VovVcCd_"},"outputs":[],"source":["#Ground Truth\n","TestVideoFile = {}\n","TestVideoFile[1] = list(range(78,84)) + list(range(95,100)) + list(range(109,111)) + list(range(116,121)) + list(range(392,400)) + list(range(408,410)) + list(range(109,111)) + list(range(503,667)) + list(range(868,876)) + list(range(878,898)) + list(range(900,911)) + list(range(932,1102)) \n","TestVideoFile[2] = list(range(273,321)) + list(range(724,765))\n","TestVideoFile[3] = list(range(295,341)) + list(range(582,623))\n","TestVideoFile[4] = list(range(380,429)) + list(range(649,693))\n","TestVideoFile[5] = range(469,787)\n","TestVideoFile[6] = list(range(345,626)) + list(range(815,826)) + list(range(856,906)) + list(range(921,1008))\n","TestVideoFile[7] = list(range(300,308)) + list(range(423,495)) + list(range(563,606))\n","TestVideoFile[8] = range(1,37)\n","TestVideoFile[9] = list(range(136,184)) + list(range(496,548)) + list(range(551,564)) + list(range(565,567)) + list(range(741,756)) + list(range(875,909)) + list(range(924,938)) + list(range(946,956)) + list(range(969,982)) + list(range(1013,1045)) + list(range(1104,1134)) + list(range(1138,1164))\n","TestVideoFile[10] = list(range(571,584)) + list(range(593,608)) + list(range(637,657)) + list(range(678,691)) + list(range(699,714)) + list(range(724,756)) + list(range(783,794)) + list(range(805,819))\n","TestVideoFile[11] = list(range(13,35)) + list(range(48,97)) + list(range(113,126)) + list(range(144,165)) + list(range(308,347))\n","TestVideoFile[12] = list(range(539,580)) + list(range(592,618)) + list(range(645,659)) + list(range(673,688)) + list(range(695,730)) + list(range(759,796)) + list(range(822,844))\n","TestVideoFile[13] = list(range(259,287)) + list(range(458,467)) + list(range(473,511))\n","TestVideoFile[14] = list(range(399,411)) + list(range(417,456)) + list(range(485,508))\n","TestVideoFile[15] = range(498,588)\n","TestVideoFile[16] = range(632,741)\n","TestVideoFile[17] = list(range(1,57)) + list(range(99,427))\n","TestVideoFile[18] = range(1,295)\n","TestVideoFile[19] = range(109,249)\n","TestVideoFile[20] = list(range(65,145)) + list(range(168,242))\n","TestVideoFile[21] = range(14,67)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NJG6mnWEh0rC","scrolled":true},"outputs":[],"source":["import os\n","from os import listdir\n","from os.path import isfile, join, isdir\n","\n","\n","from PIL import Image\n","import numpy as np\n","import shelve\n","import keras\n","import tensorflow as tf \n","from keras.layers import Conv2DTranspose, ConvLSTM2D, BatchNormalization, TimeDistributed, Conv2D, LayerNormalization\n","from keras.models import Sequential, load_model\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn import metrics\n","\n","import shutil\n","import pathlib\n","\n","import subprocess\n","\n","os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXBoGhaXbUME"},"outputs":[],"source":["#the paths need to be changed accordingly \n","class Config:\n","  VIDEO_PATH = \"/home/user/notebook/UCSD/Avenue_Dataset\"\n","  DATASET_PATH =\"/home/user/notebook/UCSD/Avenue_Dataset/training_frames\"\n","  DATASET_PATH_ALLIMAGES =\"/home/user/notebook/UCSD/Avenue_Dataset/all_images\"\n","  TEST_PATH =\"/home/user/notebook/UCSD/Avenue_Dataset/testing_frames\"\n","  SINGLE_TEST_VIDEO_FILE = 1\n","  SINGLE_TEST_PATH = \"/home/user/notebook/UCSD/Avenue_Dataset/testing_frames/01\"\n","  BATCH_SIZE = 32\n","  EPOCHS = 50\n","  MODEL_PATH = \"/home/user/notebook/drive/avenue_model_v2.hdf5\"\n","  MODEL_PATH_GEN = \"/home/user/notebook/drive/avenue_model_gen_v2.hdf5\"\n","  TRAINING_SET_PATH = \"/home/user/notebook/drive/avenue_trainingset_v2.npy\"\n","  THRESHOLD = 0.95"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qHRqRrPhwgc"},"outputs":[],"source":["#!rm -Rf UCSD/Avenue_Dataset/testing_frames\n","#!rm -Rf UCSD/Avenue_Dataset/training_frames\n","#!rm -Rf UCSD/Avenue_Dataset/test_frames\n","#!rm -Rf UCSD/Avenue_Dataset/train_frames"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I55-lu5hrLjn","scrolled":true},"outputs":[],"source":["# dividing into frames\n","\n","def videos_to_frames(root_dataset, out_path):\n","\n","    for i, (directory, folders, fil) in enumerate(os.walk(root_dataset)):\n","        for video_name in fil:\n","\n","            \n","            video_path = os.path.join(directory, video_name)\n","         \n","            fname = video_name.split('.avi')[0]\n","            path_folder = os.path.join(out_path, fname)\n","            if not os.path.isdir(path_folder):\n","                os.makedirs(path_folder)\n","\n","            \n","            change_format = 'ffmpeg -i {} -vf fps=25  {}/%04d.jpg'.format(video_path, path_folder)\n","            subprocess.call(change_format, shell=True)            \n","  \n","\n","\n","\n","# training \n","dataset_root_train = os.path.join(Config.VIDEO_PATH, 'training_videos')\n","\n","out_split_train = os.path.join(Config.VIDEO_PATH, 'training_frames')\n","if not os.path.isdir(out_split_train):\n","    os.mkdir(out_split_train)\n","\n","# testing\n","dataset_root_test = os.path.join(Config.VIDEO_PATH, 'testing_videos')\n","\n","out_split_test = os.path.join(Config.VIDEO_PATH, 'testing_frames')\n","if not os.path.isdir(out_split_test):\n","    os.mkdir(out_split_test)\n","\n","\n","\n","videos_to_frames(dataset_root_train, out_split_train)\n","videos_to_frames(dataset_root_test, out_split_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhk1v0JSZNRM","scrolled":true},"outputs":[],"source":["#Copying all frames into 1 catalog to later feed it to generator\n","\n","pathlib.Path(Config.DATASET_PATH_ALLIMAGES).mkdir(parents=True, exist_ok=True)\n","\n","for subdir, dirs, files in os.walk(Config.DATASET_PATH):\n","  for f in files:\n","    if pathlib.Path(f).suffix == \".jpg\":\n","      shutil.copy(join(Config.DATASET_PATH,subdir,f), join(Config.DATASET_PATH_ALLIMAGES, os.path.basename(subdir).lower()+\"_\"+f.lower()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ENOW0O5Bk6t3"},"outputs":[],"source":["\"\"\"\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/01 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/02 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/03 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/04 | wc -l\n","!ls /home/user/notebook//UCSD/Avenue_Dataset/training_frames/05 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/06 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/07 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/08 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/09 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/10 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/11 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/12 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/13 | wc -l\n","!ls /home/user/notebook//UCSD/Avenue_Dataset/training_frames/14 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/15 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/training_frames/16 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/all_images | wc -l\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wicE646MbgEy"},"outputs":[],"source":["def get_clips_by_stride(stride, frames_list, sequence_size):\n","  #pre-processing, reshaping, putting into volumes\n","    clips = []\n","    sz = len(frames_list)\n","    clip = np.zeros(shape=(sequence_size, 227, 227, 1))\n","    cnt = 0\n","    for start in range(0, stride):\n","        for i in range(start, sz, stride):\n","            clip[cnt, :, :, 0] = frames_list[i]\n","            cnt = cnt + 1\n","            if cnt == sequence_size:\n","                clips.append(np.copy(clip))\n","                cnt = 0\n","    return clips\n","\n","\n","def get_training_set(reload_training_set=True):\n","\n","    if not reload_training_set:\n","        return np.load(Config.TRAINING_SET_PATH)\n","    \n","    clips = []\n","    \n","    for f in sorted(listdir(Config.DATASET_PATH)):\n","        if isdir(join(Config.DATASET_PATH, f)):\n","            all_frames = []\n","            \n","            for c in sorted(listdir(join(Config.DATASET_PATH, f))):\n","                if str(join(join(Config.DATASET_PATH, f), c))[-3:] == \"tif\":\n","                    img = Image.open(join(join(Config.DATASET_PATH, f), c)).convert('L').resize((227, 227))\n","                    \n","                    img = np.array(img, dtype=np.float32)\n","                    \n","                    img=(img-img.mean())/(img.std())\n","                    \n","                    img=np.clip(img,0,1)\n","                    all_frames.append(img)\n","            \n","            for stride in range(1, 3):\n","                clips.extend(get_clips_by_stride(stride=stride, frames_list=all_frames, sequence_size=10))\n","                \n","    \n","    np.save(Config.TRAINING_SET_PATH, clips)\n","    return clips\n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BRHXG0CJh0rH"},"outputs":[],"source":["class DataGenerator(tf.keras.utils.Sequence):\n","\n","    def __init__(self, frame_list, batch_size):\n","        self.frame_list = frame_list\n","        self.batch_size = batch_size\n","\n","    def __len__(self):\n","      return (np.ceil(len(self.frame_list) / float(self.batch_size))).astype(np.int)\n","\n","    def __getitem__(self, index):\n","      frame_list_temp = self.frame_list[index * self.batch_size : (index+1) * self.batch_size]\n","\n","      # Set of X_train and y_train\n","      X, Y = self.__data_generation(frame_list_temp)\n","\n","      return X, Y\n","\n","    def __data_generation(self, frame_list_temp):\n","        frames = []\n","        for f in frame_list_temp:\n","          img = Image.open(join(Config.DATASET_PATH_ALLIMAGES, f)).convert('L').resize((227, 227))\n","\n","          img = np.array(img, dtype=np.float32)\n","          \n","          img=(img-img.mean())/(img.std())\n","\n","          \n","          img=np.clip(img,0,1)\n","          frames.append(img)\n","        \n","\n","        augmentation = []\n","        \n","        for stride in range(1, 3):\n","            augmentation.extend(get_clips_by_stride(stride=stride, frames_list=frames, sequence_size=10))\n","                \n","        training_set = np.array(augmentation)\n","        training_set = training_set.reshape(-1,10,227,227,1)\n","        \n","        return training_set, training_set.copy()\n","        \n","             \n","\n","\n","def get_model_gen(reload_model=True):\n","    if not reload_model:\n","        return load_model(Config.MODEL_PATH_GEN,custom_objects={'LayerNormalization': LayerNormalization})\n","    \n","    \n","    frames = []\n","    # loop over the all images\n","    for f in sorted(listdir(Config.DATASET_PATH_ALLIMAGES)):\n","      frames.append(f)\n","            \n","\n","    training_generator = DataGenerator(frames,Config.BATCH_SIZE)\n","\n","    seq = Sequential()\n","    \n","    \n","    seq.add(TimeDistributed(Conv2D(128, (11, 11), strides=4, padding=\"valid\"), batch_input_shape=(None, 10, 227, 227, 1)))\n","    seq.add(LayerNormalization())\n","    seq.add(TimeDistributed(Conv2D(64, (5, 5), strides=2, padding=\"valid\")))\n","    seq.add(LayerNormalization())\n","    # # # # #\n","    seq.add(ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True))\n","    seq.add(LayerNormalization())\n","    seq.add(ConvLSTM2D(32, (3, 3), padding=\"same\", return_sequences=True))\n","    seq.add(LayerNormalization())\n","    seq.add(ConvLSTM2D(64, (3, 3), padding=\"same\", return_sequences=True))\n","    seq.add(LayerNormalization())\n","    # # # # #\n","    seq.add(TimeDistributed(Conv2DTranspose(128, (5, 5), strides=2, padding=\"valid\")))\n","    seq.add(LayerNormalization())\n","    seq.add(TimeDistributed(Conv2DTranspose(1, (11, 11), strides=4, padding=\"valid\")))\n","    \n","    print(seq.summary())\n","    \n","    seq.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=1e-3)) \n","    \n","    \n","    seq.fit_generator(generator=training_generator,\n","      epochs = Config.EPOCHS,\n","      verbose = 1,\n","      max_queue_size=Config.BATCH_SIZE,\n","      callbacks=[\n","        keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5, mode=\"min\")\n","      ],)\n","\n","    \n","    seq.save(Config.MODEL_PATH_GEN)\n","    return seq\n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PtjjZx6ObpKk"},"outputs":[],"source":["def get_single_test():\n","    sz = 0\n","\n","    for subdir, dirs, files in os.walk(Config.SINGLE_TEST_PATH):\n","      for f in files:\n","        if pathlib.Path(f).suffix == \".jpg\":\n","          sz = sz + 1\n","\n","   \n","    test = np.zeros(shape=(sz, 227, 227, 1))\n","    \n","    cnt = 0\n","    for f in sorted(listdir(Config.SINGLE_TEST_PATH)):\n","        if str(join(Config.SINGLE_TEST_PATH, f))[-3:] == \"jpg\":\n","            img = Image.open(join(Config.SINGLE_TEST_PATH, f)).convert('L').resize((227, 227))\n","            img = np.array(img, dtype=np.float32)\n","            #\n","            img=(img-img.mean())/(img.std())\n","            \n","            img=np.clip(img,0,1)\n","            test[cnt, :, :, 0] = img\n","            cnt = cnt + 1\n","    return test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sjFrhXAhFU0N"},"outputs":[],"source":["#!ls /home/user/notebook//UCSD/Avenue_Dataset/testing_frames/01 | wc -l"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2NeQPTPsbsjT","scrolled":true},"outputs":[],"source":["def evaluate(reload_model=False):\n","    !free -h\n","    model = get_model_gen(reload_model)\n","    print(\"got model\")\n","    test = get_single_test()\n","    print(test.shape)\n","    sz = test.shape[0] - 10 + 1\n","    sequences = np.zeros((sz, 10, 227, 227, 1))\n","\n","    #print(\"Memory size of numpy array in bytes:\", (sequences.size * sequences.itemsize)/1024/1024, 'MB')\n","   \n","\n","    \n","    for i in range(0, sz):\n","        clip = np.zeros((10, 227, 227, 1))\n","\n","        for j in range(0, 10):\n","            clip[j] = test[i + j, :, :, :]\n","        sequences[i] = clip\n","\n","    print(\"got data\")\n","    \n","    \n","    \n","    reconstructed_sequences = []\n","\n","    index = 0\n","    batch_size = 100\n","    while (index * batch_size) < len(sequences): \n","      #print('Predict index: ', index)\n","      sequences_batch = sequences[index * batch_size : (index+1) * batch_size]\n","      reconstructed_sequences.extend(model.predict(sequences_batch, batch_size=Config.BATCH_SIZE))\n","      index = index + 1\n","      \n","      tf.keras.backend.clear_session()\n","    \n","    sequences_reconstruction_cost = np.array([np.linalg.norm(np.subtract(sequences[i],reconstructed_sequences[i])) for i in range(0,sz)])\n","    sa = (sequences_reconstruction_cost - np.min(sequences_reconstruction_cost)) / np.max(sequences_reconstruction_cost)\n","    sr = 1.0 - sa\n","\n","    # plot the regularity scores\n","    plt.plot(sr)\n","    plt.ylabel('regularity score Sr(t)')\n","    plt.xlabel('frame t')\n","    plt.show()\n","\n","    del model\n","    del test\n","    del reconstructed_sequences\n","    del sa\n","    \n","    return sr, sequences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HmRG1hJaIA8G"},"outputs":[],"source":["#!free -h"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jk3IAXWEKRQl"},"outputs":[],"source":["\"\"\"\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/01 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/02 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/03 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/04 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/05 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/06 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/07 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/08 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/09 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/10 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/11 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/12 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/13 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/14 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/15 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/16 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/17 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/18 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/19 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/20 | wc -l\n","!ls /home/user/notebook/UCSD/Avenue_Dataset/testing_frames/21 | wc -l\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvJfD_Hebwpp","scrolled":true},"outputs":[],"source":["pr, before_reconstuction = evaluate(reload_model=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qhbh8nTGd4Eo"},"outputs":[],"source":["def plotROC(pr):\n","  y_pred = pr\n","\n","  sz = 0\n","  for subdir, dirs, files in os.walk(Config.SINGLE_TEST_PATH):\n","    for f in files:\n","      if pathlib.Path(f).suffix == \".jpg\":\n","        sz = sz + 1\n","\n","  y_test = [1 for element in range(0, sz)]\n","\n","  for i in TestVideoFile[Config.SINGLE_TEST_VIDEO_FILE]:\n","    if i < len(y_test):\n","        y_test[i] = 0\n","\n","  \n","  y_test = y_test[5:sz-4]\n","\n","  \n","  if 1 not in y_test: \n","    y_test[0] = 1\n","\n","  fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n","  fnr = 1 - tpr\n","  auc = metrics.roc_auc_score(y_test, y_pred)\n","\n","  eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n","  eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n","\n","  optimal = np.argmax(tpr - fpr)\n","  optimal_threshold = thresholds[optimal]\n","\n","\n","  #print(\"FPR: \", fpr)\n","  #print(\"TPR: \", tpr)\n","  #print(\"THRESHOLDS\", thresholds)\n","  print(\"AUC: \", auc)\n","  print(\"EER: \", eer)\n","  print(\"EER THRESHOLD: \", eer_threshold)\n","  print(\"Optimal threshold value is:\", optimal_threshold)\n","\n","  plt.title('Receiver Operating Characteristic')\n","  plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n","  plt.legend(loc = 'lower right')\n","  plt.plot([0, 1], [0, 1],'r--')  \n","  plt.ylabel('True Positive Rate')\n","  plt.xlabel('False Positive Rate')\n","  plt.show()\n","\n","  plt.plot(y_test)\n","  plt.title('Labels')\n","  plt.ylabel('GT')\n","  plt.xlabel('Frame')\n","  plt.show()\n","\n","  return auc, eer\n","\n","plotROC(pr)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DseWxOg7gxVf"},"outputs":[],"source":["clips = []\n","\n","for f in sorted(listdir(Config.TEST_PATH)):\n","    if isdir(join(Config.TEST_PATH, f)):\n","      if not 'gt' in f:\n","        clips.append(join(Config.TEST_PATH, f))\n","\n","\n","scores = []\n","\n","for i in range(len(clips)):\n","\n","  Config.SINGLE_TEST_PATH = clips[i]\n","  Config.SINGLE_TEST_VIDEO_FILE = i+1\n","\n","  print(\"PATH: \", Config.SINGLE_TEST_PATH)\n","  print(\"GT: \", Config.SINGLE_TEST_VIDEO_FILE)\n","\n","  pr, before_reconstuction = evaluate()\n","  scores.append(plotROC(pr))\n","\n","\n","mean = np.mean(scores, axis=0)\n","#print(scores)\n","print(\"AUC: \", mean[0])\n","print(\"EER: \", mean[1])"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Autoencoder_with_LSTM_Avenue_dataset.ipynb","provenance":[{"file_id":"17mQ63DwtfwwMG3xJJ3a9W8pL0QoQMIYk","timestamp":1639504185138},{"file_id":"16H4c5MSlxFyuYJ4uCC_55XWZTijjMgY0","timestamp":1639434062452},{"file_id":"1uiIcCT2rKpKxU6OTkF1fPzIyQQQgsD3I","timestamp":1639425008207},{"file_id":"1r5Lq7reWrd7Py6grH2QZvDvcX4lGT8gc","timestamp":1635885601774},{"file_id":"1UUHU9oBPw1Kx7LE-M0OlxD3JSLfeUZbG","timestamp":1635094471995}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}